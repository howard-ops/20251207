{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTztPubBCHQ311FYR7U74b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/howard-ops/20251207/blob/main/20251211.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import urllib.request as req\n",
        "import bs4 as bs\n",
        "\n",
        "def download_meta(url):\n",
        "  # url = \"https://www.ptt.cc/bbs/Beauty/M.1758273800.A.03F.html\"\n",
        "  resp = req.urlopen(url)\n",
        "  content = resp.read()\n",
        "  html = bs.BeautifulSoup(content)\n",
        "\n",
        "  metas = html.find_all(\"span\", {\"class\":\"article-meta-value\"})\n",
        "  uid = metas[0]\n",
        "  board = metas[1]\n",
        "  title = metas[2]\n",
        "  posttime = metas[3]\n",
        "\n",
        "\n",
        "  uid_text = uid.get_text().strip()\n",
        "  board_text = board.get_text().strip()\n",
        "  title_text = title.get_text().strip()\n",
        "  posttime_text = posttime.get_text().strip()\n",
        "\n",
        "  pushes = html.find_all(\"div\", {\"class\":\"push\"})\n",
        "  push_list = []\n",
        "  for p in pushes:\n",
        "      trans = {\"推\":1, \"→\":0, \"噓\":-1}\n",
        "      pushtag = p.find(\"span\", {\"class\":\"push-tag\"})\n",
        "      pushuid = p.find(\"span\", {\"class\":\"push-userid\"})\n",
        "      pushcontent = p.find(\"span\", {\"class\":\"push-content\"})\n",
        "      pushipdatetime = p.find(\"span\", {\"class\":\"push-ipdatetime\"})\n",
        "\n",
        "      pushtag_text = pushtag.get_text().strip()\n",
        "      pushtag_score = trans[pushtag_text]\n",
        "      pushuid_text = pushuid.get_text().strip()\n",
        "      pushcontent_text = pushcontent.get_text().strip().replace(\": \", \"\")\n",
        "      pushipdatetime_text = pushipdatetime.get_text().strip()\n",
        "\n",
        "      push_meta = {\n",
        "          \"score\": pushtag_score,\n",
        "          \"uid\": pushuid_text,\n",
        "          \"content\": pushcontent_text,\n",
        "          \"ipdatetime\": pushipdatetime_text\n",
        "      }\n",
        "      push_list.append(push_meta)\n",
        "\n",
        "  # extract: 我都放最後做(因為刪了就刪了)\n",
        "  maincontent = html.find(\"div\", {\"id\":\"main-content\"})\n",
        "  targets = html.find_all(\"div\", {\"class\":\"article-metaline\"})\n",
        "  for t in targets:\n",
        "      t.extract()\n",
        "  targets = html.find_all(\"div\", {\"class\":\"article-metaline-right\"})\n",
        "  for t in targets:\n",
        "      t.extract()\n",
        "  targets = html.find_all(\"div\", {\"class\":\"push\"})\n",
        "  for t in targets:\n",
        "      t.extract()\n",
        "  maincontent_text = maincontent.get_text()\n",
        "\n",
        "  # userid/看板名稱/標題/時間  推虛文: 推需>/userid/內容/ipdatetime\n",
        "  row = {\n",
        "      \"uid\": uid_text,\n",
        "      \"board\": board_text,\n",
        "      \"title\": title_text,\n",
        "      \"time\": posttime_text,\n",
        "      \"content\": maincontent_text,\n",
        "      \"pushes\": push_list\n",
        "  }\n",
        "\n",
        "  dn = url.split(\"/\")[-1]\n",
        "  fn = dn + \"/\" + \"metadata.json\"\n",
        "  print(\"Save to:\", fn)\n",
        "  # 如果資料夾不存在, 就make起來\n",
        "  if not os.path.exists(dn):\n",
        "      os.makedirs(dn)\n",
        "\n",
        "  with open(fn, \"w\", encoding=\"utf-8\") as f:\n",
        "      json.dump(row, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "MJTnSJ4JNtWA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_imgs(url):\n",
        "  # url = \"https://www.ptt.cc/bbs/Beauty/M.1758273800.A.03F.html\"\n",
        "  resp = req.urlopen(url)\n",
        "  content = resp.read()\n",
        "  html = bs.BeautifulSoup(content)\n",
        "\n",
        "  allow_subs = [\"jpg\", \"jpeg\", \"png\", \"gif\", \"webp\"]\n",
        "  links = html.find_all(\"a\")\n",
        "  for l in links:\n",
        "    l_herf = l[\"href\"]\n",
        "    l_subname = l_herf.split(\".\")[-1]\n",
        "    if l_subname in allow_subs:\n",
        "      print(\"donload:\", l_herf)\n",
        "\n",
        "      r = req.Request(l_herf)\n",
        "      r.add_header(\"user-agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36\")\n",
        "      resq = req.urlopen(r)\n",
        "      content = resq.read()\n",
        "\n",
        "      dn = url.split(\"/\")[-1]\n",
        "      fn = dn + \"/\" + l_herf.split(\"/\")[-1]\n",
        "      print(\"Save to:\", fn)\n",
        "      # 如果資料夾不存在, 就make起來\n",
        "      if not os.path.exists(dn):\n",
        "        os.makedirs(dn)\n",
        "      with open(fn, \"wb\") as f:\n",
        "          f.write(content)\n",
        "    else:\n",
        "      print(\"pass:\", l_herf)"
      ],
      "metadata": {
        "id": "1wEmMiYwQRDC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.ptt.cc/bbs/Beauty/index3946.html\"\n",
        "# 走過每個超連結(拿包他的區塊)\n",
        "# 拿出href, 完整網址->定義好的兩個函式\n",
        "\n",
        "resp = req.urlopen(url)\n",
        "content = resp.read()\n",
        "html = bs.BeautifulSoup(content)\n",
        "\n",
        "links = html.find_all(\"div\", {\"class\":\"title\"})\n",
        "for l in links:\n",
        "  print(l)\n",
        "  #l_href = l[\"href\"]\n",
        "  #print(l_herf)"
      ],
      "metadata": {
        "id": "BwJ9616YiSRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.ptt.cc/bbs/Beauty/index3999.html\"\n",
        "# 走過每個超連結(拿包他的區塊)\n",
        "# 拿出href, 完整網址->定義好的兩個函式\n",
        "resp = req.urlopen(url)\n",
        "content = resp.read()\n",
        "html = bs.BeautifulSoup(content)\n",
        "\n",
        "post_divs = html.find_all(\"div\", {\"class\":\"title\"})\n",
        "for div in post_divs:\n",
        "    post_link = div.find(\"a\")\n",
        "    # == None: 裡面沒有a, 被刪文了\n",
        "    if not post_link == None:\n",
        "        post_link_href = \"https://www.ptt.cc\" + post_link[\"href\"]\n",
        "        download_meta(post_link_href)\n",
        "        download_imgs(post_link_href)\n",
        "        print(post_link_href)\n",
        "    else:\n",
        "        print(\"被刪文了!!\")"
      ],
      "metadata": {
        "id": "sgTvfO8Xt_fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "table = []\n",
        "for fn in glob.glob(\"*/metadata.json\"):\n",
        "    with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "        row = json.loads(content)\n",
        "        table.append(row)\n",
        "df = pd.DataFrame(table)\n",
        "df"
      ],
      "metadata": {
        "id": "m0oSDJ85vsuH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}